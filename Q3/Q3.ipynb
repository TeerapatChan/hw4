{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DB0vv4pBcWu9"
   },
   "source": [
    "# Q3 Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports\n",
    "Do not modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GalZFbfhcWvA"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import pkg_resources\n",
    "from pkg_resources import DistributionNotFound, VersionConflict\n",
    "from platform import python_version\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tests as tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Verify your Python version and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#function to check setup\n",
    "def check_env_setup():\n",
    "    dependencies = open(\"requirements.txt\").readlines()\n",
    "    try:\n",
    "        pkg_resources.require(dependencies)\n",
    "        print(\"✅ ALL GOOD\")\n",
    "    except DistributionNotFound as e:\n",
    "        print(\"⚠️ Library is missing\")\n",
    "        print(e)\n",
    "    except VersionConflict as e:\n",
    "        print(\"⚠️ Library version conflict\")\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Something went wrong\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Library version conflict\n",
      "(scikit-learn 1.7.2 (/Users/teerapat/working/teaching/homework/blue/HW4/Q3/.venv/lib/python3.13/site-packages), Requirement.parse('scikit-learn==0.22.1'))\n"
     ]
    }
   ],
   "source": [
    "# verify the environment setup\n",
    "check_env_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Add your Georgia Tech Username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class GaTech():\n",
    "    # Change to your GA Tech Username\n",
    "    # NOT your 9-Digit GTId\n",
    "    def GTusername(self):\n",
    "        gt_username = \"npallapangkul3\"\n",
    "        return gt_username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Z1gV3UlcWvD"
   },
   "source": [
    "# Q3.1 Data Import\n",
    "Now for the fun stuff. Let’s import some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9VS44b2kcWvE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataAllocation Function Executed\n",
      "trainSets Function Executed\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "class Data():\n",
    "    \n",
    "    # points [1]\n",
    "    def dataAllocation(self,path):\n",
    "        # TODO: Separate out the x_data and y_data and return each\n",
    "        # args: string path for .csv file\n",
    "        # return: pandas dataframe, pandas series\n",
    "        # -------------------------------\n",
    "\n",
    "        data = pd.read_csv(path)\n",
    "        y_data = data.pop('y')\n",
    "        x_data = data\n",
    "\n",
    "        return x_data,y_data\n",
    "    \n",
    "    # points [1]\n",
    "    def trainSets(self,x_data,y_data):\n",
    "        # TODO: Split 70% of the data into training and 30% into test sets. Call them x_train, x_test, y_train and y_test.\n",
    "        # Use the train_test_split method in sklearn with the parameter 'shuffle' set to true and the 'random_state' set to 614.\n",
    "        # args: pandas dataframe, pandas dataframe\n",
    "        # return: pandas dataframe, pandas dataframe, pandas series, pandas series\n",
    "        # -------------------------------\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, shuffle=True, random_state=614)\n",
    "\n",
    "        return x_train, x_test, y_train, y_test\n",
    "\n",
    "##################################################\n",
    "##### Do not add anything below this line ########\n",
    "tests.dataTest(Data)\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q09V5Ux5cWvI"
   },
   "source": [
    "# Q3.2 Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnHXBF1UcWvJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linearClassifier Function Executed\n",
      "Linear Regression Train Accuracy:  0.9597989949748744\n",
      "Linear Regression Test Accuracy:  0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "class LinearRegressionModel():\n",
    "    \n",
    "    # points [2]\n",
    "    def linearClassifier(self,x_train, x_test, y_train):\n",
    "        # TODO: Create a LinearRegression classifier and train it.\n",
    "        # args: pandas dataframe, pandas dataframe, pandas series\n",
    "        # return: numpy array, numpy array\n",
    "        # -------------------------------\n",
    "        model = LinearRegression()\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        y_predict_train = model.predict(x_train)\n",
    "        y_predict_test = model.predict(x_test)\n",
    "\n",
    "        return y_predict_train, y_predict_test\n",
    "\n",
    "    # points [1]\n",
    "    def lgTrainAccuracy(self,y_train,y_predict_train):\n",
    "        # TODO: Return accuracy (on the training set) using the accuracy_score method.\n",
    "        # Note: Round the output values greater than or equal to 0.5 to 1 and those less than 0.5 to 0. You can use any method that satisfies the requriements.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "\n",
    "        y_pred_binary = [1 if p >= 0.5 else 0 for p in y_predict_train]\n",
    "        train_accuracy = accuracy_score(y_train, y_pred_binary)\n",
    "        return train_accuracy\n",
    "    \n",
    "    # points [1]\n",
    "    def lgTestAccuracy(self,y_test,y_predict_test):\n",
    "        # TODO: Return accuracy (on the testing set) using the accuracy_score method.\n",
    "        # Note: Round the output values greater than or equal to 0.5 to 1 and those less than 0.5 to 0. You can use any method that satisfies the requriements.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "        \n",
    "        y_pred_binary = [1 if p >= 0.5 else 0 for p in y_predict_test]\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "        return test_accuracy\n",
    "    \n",
    "##################################################\n",
    "##### Do not add anything below this line ########\n",
    "tests.linearTest(Data,LinearRegressionModel)\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbqnCyHAcWvP"
   },
   "source": [
    "# Q3.3 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTtIFJW7cWvQ"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class RFClassifier():\n",
    "\n",
    "    # Q3.3.1 Classification using Random Forest\n",
    "\n",
    "    # points [2]\n",
    "    def randomForestClassifier(self, x_train, x_test, y_train):\n",
    "        # TODO: Create a RandomForestClassifier and train it. Set Random state to 614.\n",
    "        # args: pandas dataframe, pandas dataframe, pandas series\n",
    "        # return: RandomForestClassifier object, numpy array, numpy array\n",
    "        # -------------------------------\n",
    "\n",
    "        rf_clf = RandomForestClassifier(random_state=614)\n",
    "        rf_clf.fit(x_train, y_train)\n",
    "\n",
    "        y_predict_train = rf_clf.predict(x_train)\n",
    "        y_predict_test = rf_clf.predict(x_test)\n",
    "\n",
    "        return rf_clf, y_predict_train, y_predict_test\n",
    "    \n",
    "    # points [1]\n",
    "    def rfTrainAccuracy(self, y_train, y_predict_train):\n",
    "        # TODO: Return accuracy on the training set using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "        \n",
    "        train_accuracy = accuracy_score(y_train, y_predict_train)\n",
    "        return train_accuracy\n",
    "    \n",
    "    # points [1]\n",
    "    def rfTestAccuracy(self, y_test, y_predict_test):\n",
    "        # TODO: Return accuracy on the test set using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "        test_accuracy = accuracy_score(y_test, y_predict_test)\n",
    "        return test_accuracy\n",
    "    \n",
    "    # Q3.3.2 Feature Importance\n",
    "    \n",
    "    # points [1]\n",
    "    def rfFeatureImportance(self, rf_clf):\n",
    "        # TODO: Determine the feature importance as evaluated by the Random Forest Classifier.\n",
    "        # args: RandomForestClassifier object\n",
    "        # return: float array\n",
    "        # -------------------------------\n",
    "        feature_importance = rf_clf.feature_importances_\n",
    "        return feature_importance\n",
    "    \n",
    "    # points [1]\n",
    "    def sortedRFFeatureImportanceIndicies(self, rf_clf):\n",
    "        # TODO: Sort them in the descending order and return the feature numbers[0 to ...].\n",
    "        #       Hint: There is a direct function available in sklearn to achieve this. Also checkout argsort() function in Python.\n",
    "        # args: RandomForestClassifier object\n",
    "        # return: int array\n",
    "        # -------------------------------\n",
    "        sorted_indices = np.argsort(rf_clf.feature_importances_)[::-1]\n",
    "        return sorted_indices\n",
    "    \n",
    "    # Q3.3.3 Hyper-parameter Tuning\n",
    "\n",
    "    # points [1]\n",
    "    def hyperParameterTuning(self, rf_clf, x_train, y_train):\n",
    "        # TODO: Tune the hyper-parameters 'n_estimators' and 'max_depth'.\n",
    "        # Define param_grid for GridSearchCV as a dictionary\n",
    "        # args: RandomForestClassifier object, pandas dataframe, pandas series\n",
    "        # return: GridSearchCV object\n",
    "        # 'n_estimators': [4, 16, 256]\n",
    "        # 'max_depth': [2, 8, 16]\n",
    "        # -------------------------------\n",
    "        param_grid = {\n",
    "            'n_estimators': [4, 16, 256],\n",
    "            'max_depth': [2, 8, 16]\n",
    "        }\n",
    "\n",
    "        gscv_rfc = GridSearchCV(rf_clf, param_grid)\n",
    "        gscv_rfc.fit(x_train, y_train)\n",
    "\n",
    "        return gscv_rfc\n",
    "    \n",
    "    # points [1]\n",
    "    def bestParams(self, gscv_rfc):\n",
    "        # TODO: Get the best params, using .best_params_\n",
    "        # args:  GridSearchCV object\n",
    "        # return: parameter dict\n",
    "        # -------------------------------\n",
    "        best_params = gscv_rfc.best_params_\n",
    "        return best_params\n",
    "    \n",
    "    # points [1]\n",
    "    def bestScore(self,gscv_rfc):\n",
    "        # TODO: Get the best score, using .best_score_.\n",
    "        # args: GridSearchCV object\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "        best_score = gscv_rfc.best_score_\n",
    "        return best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNeOPWIpcWvg"
   },
   "source": [
    "# Q3.4 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9msZXyImcWvh"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class SupportVectorMachine():\n",
    "    \n",
    "# Q3.4.1 Pre-process\n",
    "\n",
    "    # points [1]\n",
    "    def dataPreProcess(self, x_train, x_test):\n",
    "        # TODO: Pre-process the data to standardize it, otherwise the grid search will take much longer.\n",
    "        # args: pandas dataframe, pandas dataframe\n",
    "        # return: pandas dataframe, pandas dataframe\n",
    "        # -------------------------------\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaled_x_train = scaler.fit_transform(x_train)\n",
    "        scaled_x_test = scaler.transform(x_test)\n",
    "        return scaled_x_train, scaled_x_test\n",
    "    \n",
    "# Q3.4.2 Classification\n",
    "\n",
    "    # points [1]\n",
    "    def SVCClassifier(self, scaled_x_train, scaled_x_test, y_train):\n",
    "        # TODO: Create a SVC classifier and train it. Set gamma = 'auto'\n",
    "        # args: pandas dataframe, pandas dataframe, pandas series\n",
    "        # return: numpy array, numpy array\n",
    "        # -------------------------------\n",
    "        model = SVC(gamma='auto')\n",
    "        model.fit(scaled_x_train, y_train)\n",
    "\n",
    "        y_predict_train = model.predict(scaled_x_train)\n",
    "        y_predict_test = model.predict(scaled_x_test)\n",
    "\n",
    "        return y_predict_train, y_predict_test\n",
    "    \n",
    "    # points [1]\n",
    "    def SVCTrainAccuracy(self, y_train, y_predict_train):\n",
    "        # TODO: Return accuracy on the training set using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float \n",
    "        # -------------------------------\n",
    "        train_accuracy = accuracy_score(y_train, y_predict_train)\n",
    "        return train_accuracy\n",
    "    \n",
    "    # points [1]\n",
    "    def SVCTestAccuracy(self, y_test, y_predict_test):\n",
    "        # TODO: Return accuracy on the test set using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float \n",
    "        # -------------------------------\n",
    "        test_accuracy = accuracy_score(y_test, y_predict_test)\n",
    "        return test_accuracy\n",
    "    \n",
    "# Q3.4.3 Hyper-parameter Tuning\n",
    "    \n",
    "    # points [1]\n",
    "    def SVMBestScore(self, scaled_x_train, y_train):\n",
    "        # TODO: Tune the hyper-parameters 'C' and 'kernel' (use rbf and linear).\n",
    "        # Note: Set n_jobs = -1 and return_train_score = True and gamma = 'auto'\n",
    "        # args: pandas dataframe, pandas series\n",
    "        # return: GridSearchCV object, float\n",
    "        # -------------------------------\n",
    "        svm_parameters = {\n",
    "            'kernel': ('rbf', 'linear'),\n",
    "            'C': [0.01, 0.1, 1.0]\n",
    "        }\n",
    "\n",
    "        svm_clf = SVC(gamma='auto')\n",
    "\n",
    "        svm_cv = GridSearchCV(\n",
    "            svm_clf,\n",
    "            svm_parameters,\n",
    "            n_jobs=-1,\n",
    "            return_train_score=True\n",
    "        )\n",
    "\n",
    "        svm_cv.fit(scaled_x_train, y_train)\n",
    "\n",
    "        best_score = svm_cv.best_score_\n",
    "        \n",
    "        return svm_cv, best_score\n",
    "    \n",
    "    # points [1]\n",
    "    def SVCClassifierParam(self, svm_cv, scaled_x_train, scaled_x_test, y_train):\n",
    "                # TODO: Calculate the training and test set predicted values after hyperparameter tuning and standardization.\n",
    "# args: GridSearchCV object, pandas dataframe, pandas dataframe, pandas series\n",
    "        # return: numpy series, numpy series\n",
    "        # -------------------------------\n",
    "        best_model = svm_cv.best_estimator_\n",
    "        y_predict_train = best_model.predict(scaled_x_train)\n",
    "        y_predict_test = best_model.predict(scaled_x_test)\n",
    "        return y_predict_train, y_predict_test\n",
    "\n",
    "    # points [1]\n",
    "    def svcTrainAccuracy(self, y_train, y_predict_train):\n",
    "        # TODO: Return accuracy (on the training set) using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "        train_accuracy = accuracy_score(y_train, y_predict_train)\n",
    "        return train_accuracy\n",
    "\n",
    "    # points [1]\n",
    "    def svcTestAccuracy(self, y_test, y_predict_test):\n",
    "        # TODO: Return accuracy (on the test set) using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "        test_accuracy = accuracy_score(y_test, y_predict_test)\n",
    "        return test_accuracy\n",
    "    \n",
    "# Q3.4.4 Cross Validation Results\n",
    "\n",
    "    # points [1]\n",
    "    def SVMRankTestScore(self, svm_cv):\n",
    "        # TODO: Return the rank test score for all hyperparameter values that you obtained in Q3.4.3. The \n",
    "        # GridSearchCV class holds a 'cv_results_' dictionary that should help you report these metrics easily.\n",
    "        # args: GridSearchCV object \n",
    "        # return: int array\n",
    "        # -------------------------------\n",
    "        rank_test_score = svm_cv.cv_results_['rank_test_score']\n",
    "        return rank_test_score\n",
    "    \n",
    "    # points [1]\n",
    "    def SVMMeanTestScore(self, svm_cv):\n",
    "        # TODO: Return mean test score for all of hyperparameter values that you obtained in Q3.4.3. The \n",
    "        # GridSearchCV class holds a 'cv_results_' dictionary that should help you report these metrics easily.\n",
    "        # args: GridSearchCV object\n",
    "        # return: float array\n",
    "        # -------------------------------\n",
    "        mean_test_score = svm_cv.cv_results_['mean_test_score']\n",
    "        return mean_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2qDYMjgcWv5"
   },
   "source": [
    "# Q3.5 PCA\n",
    "\n",
    "1. Perform dimensionality reduction of the data using PCA.  \n",
    "2. Return Explained Variance Ratios. \n",
    "3. Report Singular Values. \n",
    "4. Report how many principal components (PCs) are needed to reach ≥ 90% cumulative explained variance.  \n",
    "5. Report the top 5 feature names that contribute to PC1 (by absolute coefficient magnitude).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-C9BuGsqcWv5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcaClassifier Function Executed\n",
      "PCA Explained Variance Ratio:  [9.82044672e-01 1.61764899e-02 1.55751075e-03 1.20931964e-04\n",
      " 8.82724536e-05 6.64883951e-06 4.01713682e-06 8.22017197e-07]\n",
      "PCA Singular Values:  [1.58766659e+04 2.03767928e+03 6.32279658e+02 1.76183095e+02\n",
      " 1.50524184e+02 4.13110857e+01 3.21108643e+01 1.45256018e+01]\n",
      "Number of components to reach >=90% variance:  1\n",
      "Top 5 feature names that contribute to PC1:  ['area_worst', 'area_mean', 'area_se', 'perimeter_worst', 'perimeter_mean']\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "class PCAClassifier():\n",
    "\n",
    "    def __init__(self, random_state: int = 0):\n",
    "        self.random_state = random_state\n",
    "        self.scaler_ = None\n",
    "        self.pca_ = None\n",
    "        self.feature_names_ = None\n",
    "        self.X_scaled_ = None\n",
    "    \n",
    "    # Q3.5.1 PCA \n",
    "    # points [1]\n",
    "    def pcaClassifier(self,x_data):\n",
    "        # TODO: Perform dimensionality reduction of the data using PCA.\n",
    "        #       Set parameters n_components to 8 and svd_solver to 'full'. Keep other parameters at their default value.\n",
    "        # args: pandas dataframe\n",
    "        # return: pca_object\n",
    "        pca = PCA(n_components=8, svd_solver='full')\n",
    "        pca.fit(x_data)\n",
    "\n",
    "        self.pca_ = pca\n",
    "        self.feature_names_ = list(x_data.columns)\n",
    "        self.X_scaled_ = x_data\n",
    "        return pca\n",
    "    \n",
    "    # Q3.5.2 PCA Explained Variance Ratio\n",
    "    # points [1]\n",
    "    def pcaExplainedVarianceRatio(self, pca):\n",
    "        # TODO: Return percentage of variance explained by each of the selected components\n",
    "        # args: pca_object\n",
    "        # return: float array\n",
    "        # -------------------------------\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        return explained_variance_ratio\n",
    "    \n",
    "    # Q3.5.3 PCA Singular Values\n",
    "    # points [1]\n",
    "    def pcaSingularValues(self, pca):\n",
    "        # TODO: Return the singular values corresponding to each of the selected components.\n",
    "        # args: pca_object\n",
    "        # return: float array\n",
    "        # -------------------------------\n",
    "        singular_values = pca.singular_values_\n",
    "        return singular_values\n",
    "    \n",
    "    # Q3.5.4 Cumulative Explained Variance\n",
    "    # points [1]\n",
    "    def n_components_for_variance(self, pca, threshold=0.90) -> int:\n",
    "        # TODO: how many PCs to reach given cumulative explained variance threshold?\n",
    "        # args: pca_object\n",
    "        # return: int\n",
    "        cum = np.cumsum(pca.explained_variance_ratio_)\n",
    "        n_components = int(np.argmax(cum >= threshold) + 1)\n",
    "        return n_components\n",
    "    # Q3.5.5 Top Feature Contributors to PC1\n",
    "    # points [1]\n",
    "    def top_pc1_contributors(self, pca, top_n: int = 5):\n",
    "        # TODO: Return the top-N feature names contributing to PC1 by |coefficient|.\n",
    "        # Deterministic ordering: sort by abs(coef) desc; ties by name asc.\n",
    "        # args: pca_object\n",
    "        # return: list of strings\n",
    "        pc1 = pca.components_[0]\n",
    "        names = getattr(pca, \"feature_names_in_\", None)\n",
    "        if names is None:\n",
    "            names = self.feature_names_\n",
    "\n",
    "        pairs = []\n",
    "        for i in range(len(names)):\n",
    "            name = names[i]\n",
    "            coef = abs(pc1[i])\n",
    "            pairs.append((name, coef))\n",
    "\n",
    "        pairs.sort(key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "        top_features = [pairs[i][0] for i in range(top_n)]\n",
    "        return top_features\n",
    "    \n",
    "##################################################\n",
    "##### Do not add anything below this line ########\n",
    "tests.PCATest(Data,PCAClassifier)\n",
    "##################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw4q3.soln.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
